{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Video enhanced classification V2\n",
    "# does away with first classification to find areas of detection\n",
    "# assumes videographer places objects of interest in the visual center of attention\n",
    "\n",
    "# Experiment 1b - use variations in luminance to generate varied images with constant content\n",
    "# Pass to bank of 'imperfectly' pretrained classifiers (they tend to only find one type of plant)\n",
    "# 'Zoom' into area where imperfect classifiers find some 'action'\n",
    "# Pass those image zones to the classifiers bank and collect the varied results\n",
    "# Combine results to get most likely set of multiple plants\n",
    "# Option: use audio from video to confirm.\n",
    "# March 2020, RTS\n",
    "#----------------------------------------------------------------------------------------------\n",
    "import argparse\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os, sys\n",
    "import time\n",
    "import cv2\n",
    "import tqdm\n",
    "import json\n",
    "import numpy\n",
    "from  matplotlib import pyplot as plt\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.checkpoint import DetectionCheckpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add helper file\n",
    "sys.path.append('/home/rts/code/')\n",
    "from similarities import *\n",
    "from av_helper import *\n",
    "from utilities_v2 import *\n",
    "\n",
    "#set data paths\n",
    "datapath = '/home/rts/data/'\n",
    "savepath = datapath + 'tempimgs/'\n",
    "path2model = '/home/rts/detectron2/demo/output/'\n",
    "name = \"labelled_94imgs\"\n",
    "jsonpath = '/home/rts/data/labelled_94imgs.json'\n",
    "imagespath = '/home/rts/data/labelled_94imgs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 26\n",
    "with open(jsonpath) as jsonfile:\n",
    "    cocodata = json.load(jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "register_coco_instances(name, {}, jsonpath, imagespath)\n",
    "bali26_boxl_metadata = MetadataCatalog.get(name)\n",
    "\n",
    "dataset_dicts = DatasetCatalog.get(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START the processing\n",
    "# get the video and collect stats on luminance (or perceived_luminance, or timing)\n",
    "\n",
    "import numpy\n",
    "\n",
    "#delete the images in the tempimg folder\n",
    "try:\n",
    "  shutil.rmtree(savepath)\n",
    "  print('emptying the search directory')\n",
    "except:\n",
    "  pass\n",
    "\n",
    "print('recreating directory: ', savepath)\n",
    "os.makedirs(savepath)\n",
    "\n",
    "#set the video  name\n",
    "videoname = 'banana+cacao.mp4'            \n",
    "groundtruth = videoname.split('.mp4')[0]\n",
    "\n",
    "l = get_video_length(datapath+videoname)\n",
    "print('Length of test video: ', l)\n",
    "\n",
    "# make images\n",
    "category = 'blah'\n",
    "framerate = 3\n",
    "videopath = datapath + videoname\n",
    "create_images_from_video(savepath, category, videopath, framerate)\n",
    "\n",
    "#get luminance stats in a dict, then get max exposed, min exposed, ave exposed\n",
    "lum_data = []\n",
    "p_lum_data = []\n",
    "lums = []\n",
    "\n",
    "p_lums = []\n",
    "lum_dict = {}\n",
    "p_lum_dict = {}\n",
    "\n",
    "for r, d, f in os.walk(savepath):\n",
    "    for file in f:\n",
    "        lum = int(brightness(savepath + file))\n",
    "        #p_lum = int(perceived_brightness(savepath + file))\n",
    "        lum_dict = {'imagename' : file, 'luminosity' : lum}\n",
    "        #p_lum_dict = {'imagename' : file, 'perceived_luminosity' : p_lum}\n",
    "        lums.append(lum)\n",
    "        #p_lums.append(p_lum)\n",
    "        lum_data.append(lum_dict)\n",
    "        #p_lum_data.append(p_lum_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lum = int(numpy.mean(lums))\n",
    "max_lum = numpy.max(lums)\n",
    "min_lum = numpy.min(lums)\n",
    "\n",
    "max_item = next(item for item in lum_data if item[\"luminosity\"] == max_lum)\n",
    "min_item = next(item for item in lum_data if item[\"luminosity\"] == min_lum)\n",
    "\n",
    "try:\n",
    "    mean_item = next(item for item in lum_data if item[\"luminosity\"] == mean_lum)\n",
    "except:\n",
    "    print('fixing rounding..')\n",
    "    mean_lum = mean_lum-1\n",
    "    mean_item = next(item for item in lum_data if item[\"luminosity\"] == mean_lum)\n",
    "    \n",
    "print('Here are the stats')\n",
    "print(max_item)\n",
    "print(min_item)\n",
    "print(mean_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find images with min, max and mean luminance values\n",
    "# (alternative: image at start, middle and end of video....)\n",
    "# the classifiers respond differently to them...they also show different 'scenes' \n",
    "# as they occur in different parts of the video - but they contain the ++same semantics (aka plant types)++\n",
    "max_lum_img = (max_item['imagename'])\n",
    "min_lum_img = (min_item['imagename'])\n",
    "mean_lum_img = (mean_item['imagename'])\n",
    "\n",
    "valdataset = [max_lum_img, min_lum_img, mean_lum_img]\n",
    "\n",
    "print('Max, min and mean luminosity images: ', max_lum_img, min_lum_img, mean_lum_img)\n",
    "print('Number of examples in this collection: ', len(valdataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get center of attention\n",
    "portrait = False\n",
    "landscape = False\n",
    "\n",
    "for img in (valdataset):\n",
    "    im = cv2.imread(savepath+img)\n",
    "    height, width, channels = im.shape\n",
    "    if(height > width):\n",
    "        portrait = True\n",
    "    else:\n",
    "        landscape = True\n",
    "\n",
    "if(portrait == True):\n",
    "    margin = height/4\n",
    "    print('portrait style')\n",
    "    mid = int(height/2)\n",
    "    cut_top = int(mid - margin)\n",
    "    cut_bottom = int(mid + margin)\n",
    "    print('Boundaries: ', cut_top, cut_bottom) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the cropped images\n",
    "for img in (valdataset):\n",
    "    im = cv2.imread(savepath+img)\n",
    "    height, width, channels = im.shape\n",
    "    \n",
    "    if(portrait == True):\n",
    "         crop_img = im[int(cut_top):int(cut_bottom), 0:width]\n",
    "            \n",
    "    img_rgb = cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of all the trained models (94 labeled images)\n",
    "model_list = ['model_faster_rcnn_R_101_C4_3x_labeled_94imgs.pth', 'model_faster_rcnn_R_101_FPN_3x_labeled_94imgs.pth', \\\n",
    "              'model_faster_rcnn_R_50_C4_1x_labeled_94imgs.pth', 'model_faster_rcnn_R_50_FPN_noaug_1x_labeled_94imgs.pth', \\\n",
    "              'model_faster_rcnn_X_101_32x8d_FPN_3x_labeled_94imgs.pth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the collection of classifiers on the cropped images selected by variation in luminosity\n",
    "# and sort the final list, leaving out single instances\n",
    "\n",
    "names = bali26_boxl_metadata.thing_classes\n",
    "totals = {}\n",
    "\n",
    "for choice in model_list:\n",
    "    modelchoice = choice\n",
    "    modelpath = path2model + modelchoice\n",
    "    modelzoo_n = modelchoice.split('model_')\n",
    "    modelzoo_nn = modelzoo_n[1].split('_labeled_94imgs')[0] +'.yaml'\n",
    "    \n",
    "    if(choice == 'model_faster_rcnn_R_50_FPN_noaug_1x_labeled_94imgs.pth'):\n",
    "        prefix = 'Detectron1-Comparisons'\n",
    "    else:\n",
    "        prefix = 'COCO-Detection'\n",
    "    \n",
    "    modelzoo_choice = os.path.join(prefix, modelzoo_nn)\n",
    "    \n",
    "    cfg = get_cfg()\n",
    "    modelzoo_name = modelzoo_choice\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(modelzoo_name))\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5                                          \n",
    "    cfg.MODEL.WEIGHTS =  modelpath                                             \n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   \n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES =   num_classes                                                     \n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    \n",
    "    combolist = []\n",
    "\n",
    "    for img in (valdataset):\n",
    "        im = cv2.imread(savepath+img)\n",
    "        height, width, channels = im.shape\n",
    "\n",
    "        if(portrait == True):\n",
    "             crop_img = im[int(cut_top):int(cut_bottom), 0:width]\n",
    "        elif(landscape == True):\n",
    "            crop_img = im[0:height, int(cut_left):int(cut_right)]\n",
    "        else:\n",
    "            crop_img = im\n",
    "            \n",
    "        img_rgb = cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB)\n",
    "        #original_image = Image.open(savepath+img, mode='r').convert('RGB')\n",
    "        outputs = predictor(img_rgb)\n",
    "        boxes = outputs[\"instances\"].get_fields()[\"pred_boxes\"].tensor.to(\"cpu\").numpy()\n",
    "        boxes = numpy.around(boxes,decimals=0)\n",
    "        scores = outputs[\"instances\"].get_fields()[\"scores\"].squeeze().tolist()\n",
    "        scores = numpy.around(scores,decimals=4)\n",
    "        predicted_classe_indexes = outputs[\"instances\"].get_fields()[\"pred_classes\"].squeeze().tolist()\n",
    "        collection = index2name(predicted_classe_indexes, names)\n",
    "\n",
    "        scores_list = []\n",
    "        templist = []\n",
    "    \n",
    "        if(len(collection) > 0):\n",
    "            if (len(collection) == 1):\n",
    "                scores_list.append(scores)\n",
    "        else:\n",
    "            scores_list = scores\n",
    "            \n",
    "        imgname = img.split('.jpg')[0]\n",
    "        templist.append(imgname)\n",
    "        templist.append(collection)\n",
    "        templist.append(scores)\n",
    "        combolist.append(templist)\n",
    "\n",
    "    print('\\nGround truth: ', groundtruth)\n",
    "    print('Classifier: ', modelzoo_choice)\n",
    "    print('Confidence threshold: ', cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST)\n",
    "    print('Results: ', combolist) \n",
    "    \n",
    "        \n",
    "    #------------------------------------ --------------------------\n",
    "    # collect the evidence; use instances above threshold to 'vote'\n",
    "    #---------------------------------------------------------------\n",
    "    \n",
    "    hits = []\n",
    "    unique_hits = []\n",
    "    \n",
    "    for probs in combolist:\n",
    "        for item in probs[1]:\n",
    "            hits.append(item)\n",
    "            \n",
    "    if(not hits):\n",
    "        pass\n",
    "    else:\n",
    "        unique_hits = unique(hits)\n",
    "\n",
    "    for uni in unique_hits:\n",
    "        n = hits.count(uni)\n",
    "        print(uni, n)\n",
    "        k = totals.get(uni)\n",
    "        if(k):\n",
    "            nn = int(k)+n\n",
    "        else:\n",
    "            nn = n\n",
    "        totals.update({uni:nn})\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# final tally\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "total = []\n",
    "for plant, occurance in totals.items():  \n",
    "    if occurance > 1:\n",
    "        total.append(plant)\n",
    "\n",
    "print('\\nFINAL TALLY: ', totals)\n",
    "print(' > FINAL TALLY above single instance ', total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
